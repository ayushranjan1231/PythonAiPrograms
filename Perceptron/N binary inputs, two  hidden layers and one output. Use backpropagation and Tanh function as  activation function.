import numpy as np

# Tanh activation function
def tanh(x):
    return np.tanh(x)

# Derivative of Tanh function
def tanh_derivative(x):
    return 1 - np.tanh(x) ** 2

# Function to train the MLP
def train_mlp(inputs, outputs, input_size, hidden_size1, hidden_size2, output_size, learning_rate=0.1, epochs=10000):
    np.random.seed(1)

    # Initialize weights and biases
    w1 = np.random.randn(input_size, hidden_size1)
    b1 = np.random.randn(hidden_size1)
    w2 = np.random.randn(hidden_size1, hidden_size2)
    b2 = np.random.randn(hidden_size2)
    w3 = np.random.randn(hidden_size2, output_size)
    b3 = np.random.randn(output_size)

    # Training loop
    for epoch in range(epochs):
        # --- Forward pass ---
        z1 = np.dot(inputs, w1) + b1
        a1 = tanh(z1)

        z2 = np.dot(a1, w2) + b2
        a2 = tanh(z2)

        z3 = np.dot(a2, w3) + b3
        output = tanh(z3)  # Tanh at output layer too

        # --- Error calculation ---
        error = outputs - output

        if (epoch + 1) % 1000 == 0:
            print(f"Epoch {epoch + 1}, Error: {np.mean(np.abs(error))}")

        # --- Backward pass (backpropagation) ---
        output_delta = error * tanh_derivative(z3)
        a2_delta = output_delta.dot(w3.T) * tanh_derivative(z2)
        a1_delta = a2_delta.dot(w2.T) * tanh_derivative(z1)

        # --- Update weights and biases ---
        w3 += a2.T.dot(output_delta) * learning_rate
        b3 += np.sum(output_delta, axis=0) * learning_rate
        w2 += a1.T.dot(a2_delta) * learning_rate
        b2 += np.sum(a2_delta, axis=0) * learning_rate
        w1 += inputs.T.dot(a1_delta) * learning_rate
        b1 += np.sum(a1_delta, axis=0) * learning_rate

    print("\nFinal weights and biases:")
    print("w1:\n", w1)
    print("b1:\n", b1)
    print("w2:\n", w2)
    print("b2:\n", b2)
    print("w3:\n", w3)
    print("b3:\n", b3)
    print("\nTraining complete.")

# Example usage
if __name__ == "__main__":
    # N binary inputs
    inputs = np.array([
        [0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 0, 1, 1],
        [0, 1, 0, 0], [0, 1, 0, 1], [0, 1, 1, 0], [0, 1, 1, 1],
        [1, 0, 0, 0], [1, 0, 0, 1], [1, 0, 1, 0], [1, 0, 1, 1],
        [1, 1, 0, 0], [1, 1, 0, 1], [1, 1, 1, 0], [1, 1, 1, 1]
    ])

    # Corresponding outputs
    outputs = np.array([
        [0], [1], [1], [0],
        [1], [0], [0], [1],
        [0], [1], [1], [0],
        [1], [0], [0], [1]
    ])

    input_size = inputs.shape[1]
    hidden_size1 = 4
    hidden_size2 = 4
    output_size = outputs.shape[1]

    train_mlp(inputs, outputs, input_size, hidden_size1, hidden_size2, output_size)
